# backend/app/Domain/retrieval/search_service.py

import os
import re
import json
from typing import List, Dict, Any

from dotenv import load_dotenv

from engine.rag_engine import get_rag_context  # ğŸ”¹ ê³µìš© RAG ì—”ì§„ ì‚¬ìš©

load_dotenv()

# ========== â‘  ê²½ë¡œ ì„¸íŒ… ==========
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LEX_PATH = os.path.join(BASE_DIR, "risk_lexemes.jsonl")
PAT_PATH = os.path.join(BASE_DIR, "risk_patterns.jsonl")


# ========== â‘¡ ìš•ì„¤ ì‚¬ì „ ë¡œë“œ ==========
def load_lexemes(path: str) -> List[str]:
    words: List[str] = []
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    item = json.loads(line)
                    if item.get("category") == "profanity":
                        words.append(item["term"])
                except Exception:
                    continue
    return words


def load_patterns(path: str) -> List[str]:
    patterns: List[str] = []
    if os.path.exists(path):
        with open(path, "r", encoding="utf-8") as f:
            for line in f:
                try:
                    item = json.loads(line)
                    if item.get("category") == "profanity":
                        patterns.append(item["pattern"])
                except Exception:
                    continue
    return patterns


LEXEMES = load_lexemes(LEX_PATH)
PATTERNS = load_patterns(PAT_PATH)
print(f"ğŸš€ Loaded {len(LEXEMES)} lexemes, {len(PATTERNS)} patterns.")


# ========== â‘¢ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬ (fallback, í•„ìš” ì‹œ ì‚¬ìš©) ==========
def clean_text(text: str) -> str:
    text = re.sub(r"\(ì „í™”ë²ˆí˜¸[^)]*\)", "", text)
    text = re.sub(r"[A-Za-z]\s*ë¬¸ì", "", text)
    text = re.sub(r"[A-Za-z]\d+", "", text)
    text = re.sub(r"\d{2,4}[:.\-]\d{2,4}", "", text)
    text = re.sub(r"\s+", " ", text).strip()
    return text


# ========== â‘£ ìš• íƒì§€ ==========
def detect_profanity(query: str) -> List[str]:
    hits: List[str] = []
    clean_query = re.sub(r"[\sÂ·â™¡â¤ğŸ’–â­ï¸\*]+", "", query.lower())

    for w in LEXEMES:
        if re.search(rf"{re.escape(w)}", clean_query):
            hits.append(w)

    for p in PATTERNS:
        if re.search(p, clean_query):
            hits.append(p)

    # TODO: â€œì‹œë°œâ€ â†” â€œì”¨ë°œâ€ ê°™ì€ ë³€í˜• ë§¤í•‘ì„ ë„£ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸°ì— ì¶”ê°€
    return list(set(hits))


# ========== â‘¤ ê²€ìƒ‰ (pgvector RAG + ìš•ì„¤ í•„í„° ë˜í•‘) ==========
def search_jsonl(query: str, top_k: int = 5) -> Dict[str, Any]:
    """
    - engine.rag_engine.get_rag_context() ë¥¼ í˜¸ì¶œí•´ì„œ pgvector RAG ê²€ìƒ‰ì„ ìˆ˜í–‰í•˜ê³ 
    - ìš•ì´ ì„ì¸ ì¿¼ë¦¬ë¼ë©´, ìš•ì´ ë“¤ì–´ê°„ ë¬¸ì¥ ìœ„ì£¼ë¡œ í•„í„°ë§í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œë¡œ ë™ì‘
    - ìµœì¢… ê²°ê³¼ëŠ” ê¸°ì¡´ê³¼ ê°™ì€ í˜•íƒœ:
      {"results": [{"score": ..., "source_file": ..., "chunk_text": ...}, ...]}
    """
    print(f"[í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬]: {query}")
    bad_hits = detect_profanity(query)
    is_profanity_query = len(bad_hits) > 0
    print(f"  -> ê°ì§€ëœ ìš•: {bad_hits if bad_hits else 'ì—†ìŒ'}")
    print(f"  -> ëª¨ë“œ: {'âš ï¸ ìš•ì„¤ íƒì§€ ëª¨ë“œ' if is_profanity_query else 'ğŸ’¬ ì¼ë°˜ ê²€ìƒ‰ ëª¨ë“œ'}")

    # ğŸ”¹ RAG ì»¨í…ìŠ¤íŠ¸ (pgvector + OpenAI ì„ë² ë”©) í˜¸ì¶œ
    #    ìš• í•„í„°ë§ì„ ìœ„í•´ ì—¬ìœ  ìˆê²Œ top_k*3 ì •ë„ ê°€ì ¸ì˜¤ê³  ë‚˜ì¤‘ì— ë‹¤ì‹œ ìŠ¬ë¼ì´ì‹±
    raw_contexts = get_rag_context(query, top_k=max(top_k * 3, top_k))

    if not raw_contexts:
        print("âš ï¸ RAG ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ì–´ ê¸°ë³¸ ë²•ë ¹ ë°˜í™˜.")
        return {
            "results": [
                {
                    "score": 1.0,
                    "source_file": "default",
                    "chunk_text": (
                        "í˜•ë²• ì œ311ì¡°(ëª¨ìš•) ê³µì—°íˆ ì‚¬ëŒì„ ëª¨ìš•í•œ ìëŠ” "
                        "1ë…„ ì´í•˜ì˜ ì§•ì—­ ë˜ëŠ” 200ë§Œì› ì´í•˜ì˜ ë²Œê¸ˆì— ì²˜í•œë‹¤."
                    ),
                }
            ]
        }

    # ğŸ”¹ ìš•ì„¤ ì¿¼ë¦¬ì¸ ê²½ìš°, ìš•ì´ í¬í•¨ëœ ë¬¸ì¥ë§Œ í•„í„°ë§
    contexts = raw_contexts
    if is_profanity_query:
        filtered: List[Dict[str, Any]] = []
        for c in raw_contexts:
            text = (c.get("text") or "").lower()
            if any(w in text for w in bad_hits):
                filtered.append(c)

        if filtered:
            contexts = filtered
        else:
            # ìš• ê°ì§€ëŠ” ëì§€ë§Œ, RAG ê²°ê³¼ì— ìš•ì´ ì•ˆ ë“¤ì–´ê°„ ê²½ìš° â†’ ê·¸ëƒ¥ RAG ê²°ê³¼ ì‚¬ìš©
            contexts = raw_contexts

    # ğŸ”¹ ìµœì¢… top_kë§Œ ì˜ë¼ì„œ ê¸°ì¡´ ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ë³€í™˜
    top_contexts = contexts[:top_k]

    top_results = [
        {
            "score": float(c.get("score", 0.0)),
            "source_file": c.get("source_file", "unknown"),
            "chunk_text": c.get("text", ""),
        }
        for c in top_contexts
    ]

    print("2. RAG ê²€ìƒ‰ ê²°ê³¼ (ìƒìœ„ ê²°ê³¼):")
    for r in top_results:
        print(f"   - (ìœ ì‚¬ë„ {r['score']:.4f}) {r['chunk_text'][:80]}...")

    return {"results": top_results}


print("âœ… Using engine.rag_engine(pgvector) + local profanity hybrid search model.")
